# app_optimizer.py
import streamlit as st

from pipeline import FakeLLM, OllamaLLM, LLMConfig, PipelineInput, Pipeline
from prompts.optimizer_smart import smart_optimize_prompt, estimate_tokens

st.set_page_config(page_title="Token & Prompt Optimizer", page_icon="üßÆ", layout="wide")
st.title("üßÆ Token & Prompt Optimizer")

# --- Entr√©es ---
c1, c2 = st.columns(2)
with c1:
    st.subheader("üéØ Prompt d‚Äôentr√©e")
    raw = st.text_area("Colle ici ton prompt (avant optimisation)", height=220, placeholder="Ton prompt‚Ä¶")

with c2:
    st.subheader("‚ú® Prompt optimis√©")
    optimized_box = st.empty()  # on remplira apr√®s clic

st.divider()

# Param√®tres
with st.sidebar:
    st.header("Param√®tres")
    mode_intelligent = st.toggle("Activer l‚Äôoptimisation intelligente (Ollama)", value=True,
                                 help="R√©√©criture par LLM (Mistral via Ollama). Sinon, mode m√©canique.")
    target_tokens = st.slider("Cible (tokens entr√©e)", 32, 512, 128, 16)
    max_words     = st.slider("Contrainte max mots (hint)", 30, 300, 120, 10)
    temperature   = st.slider("Temp√©rature LLM", 0.0, 1.0, 0.2, 0.1)

    # Config Ollama
    model_name = st.text_input("Mod√®le Ollama", "mistral")
    endpoint   = st.text_input("Endpoint", "http://localhost:11434/api/generate")
    st.caption("D√©marre `ollama serve` et `ollama pull mistral` si besoin.")

# Bouton
if st.button("üöÄ Optimiser"):
    if not raw.strip():
        st.warning("Ajoute un prompt.")
        st.stop()

    if mode_intelligent:
        try:
            llm = OllamaLLM(model=model_name, endpoint=endpoint)
            res = smart_optimize_prompt(
                llm=llm,
                raw_prompt=raw,
                target_tokens=target_tokens,
                max_words=max_words,
                temperature=temperature,
                max_tokens=256,
            )
            optimized_box.text_area("Version optimis√©e", res.optimized, height=220)
            st.subheader("üìä Statistiques")
            c3, c4, c5 = st.columns(3)
            c3.metric("Tokens (entr√©e) AVANT", res.tokens_in)
            c4.metric("Tokens (entr√©e) APR√àS",  res.tokens_out, delta=-res.gain_tokens if res.gain_tokens>0 else res.tokens_out - res.tokens_in)
            c5.metric("Gain tokens (entr√©e)",   res.gain_tokens)

        except Exception as e:
            st.error(f"√âchec de l‚Äôoptimisation intelligente : {e}")
    else:
        # Fallback M√©canique minimal : nettoie politesse + ajoute contrainte
        import re
        prompt = raw.strip()
        # Supprime salutations courantes
        prompt = re.sub(r"(?i)\b(bonjour|salut|merci|s'il te pla√Æt|svp|stp)\b[:,\s]*", "", prompt)
        prompt = prompt.strip()
        # Ajoute contrainte de concision (si pas d√©j√† l√†)
        if "r√©ponds en" not in prompt.lower():
            prompt += f"\n\nContrainte : r√©ponds en points concis (‚â§ {max_words} mots)."
        optimized_box.text_area("Version optimis√©e", prompt, height=220)

        t_in  = estimate_tokens(raw)
        t_out = estimate_tokens(prompt)
        st.subheader("üìä Statistiques")
        c3, c4, c5 = st.columns(3)
        c3.metric("Tokens (entr√©e) AVANT", t_in)
        c4.metric("Tokens (entr√©e) APR√àS", t_out, delta=t_out - t_in)
        c5.metric("Gain tokens (entr√©e)", t_in - t_out)

st.info("Astuce : colle d‚Äôabord ton prompt ici pour le r√©duire, puis utilise-le dans l‚Äôapp principale.")